name: Remote deployment

on:   
  push:
    branches:
      - master
      - develop
    paths:
      - '**.py'
      - '**/Dockerfile'
      - '**/requirements*.txt'
      - '**/.github/**.yaml'

  pull_request:
    paths:
      - '**.py'
      - '**/Dockerfile'
      - '**/requirements*.txt'
      - '**/.github/**.yaml'

  workflow_dispatch:

jobs:
    remote-deploy:
      runs-on: ubuntu-20.04
      
      permissions:
        packages: write
        contents: read

      steps:
        - uses: actions/checkout@v2

        - name: Cache dvc and pip
          uses: actions/cache@v2
          with:
            path: |
              ~/.cache/pip
              ${{ github.workspace }}/.dvc/cache
            key: ${{ runner.os }}-pip-${{ hashFiles('requirements*txt') }}
            restore-keys: |
              ${{ runner.os }}-pip-
              ${{ runner.os }}-

        - name: Install dependencies
          run: |
            pip install -r ./requirements.dev.txt

        - name: Test that all data in the remote cache
          env:
            GDRIVE_CREDENTIALS_DATA: ${{ secrets.GDRIVE_DVC_DATA_SECRET }}
            S3_KEY_ID: ${{ secrets.S3_KEY_ID }}
            S3_ACCESS_KEY: ${{ secrets.S3_ACCESS_KEY }}
          run: |
            cat << EOF > ./.dvc/config.local
            ['remote "remote-s3"']
              access_key_id = ${S3_KEY_ID}
              secret_access_key = ${S3_ACCESS_KEY}
            EOF

            dvc pull

        - name: Install SSH key
          uses: shimataro/ssh-key-action@v2
          with:
            key: ${{ secrets.SSH_KEY }}
            name: ssh_key
            known_hosts: 'null'
            if_key_exists: fail # replace / ignore / fail; optional (defaults to fail)
        
        - name: Test ssh
          run: ssh -o StrictHostKeyChecking=no -i ~/.ssh/ssh_key ${{ secrets.SSH_USERNAME }}@${{ secrets.SSH_HOST }} ls
        
    