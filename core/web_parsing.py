import os
import requests
import re  # we import them into
from bs4 import BeautifulSoup


def extract_subscribers(soup: BeautifulSoup):
    try:
        # here we find the right tag with subcsribers
        html_tag_with_subscribers = soup.find("div", {"class": "tgme_page_extra"})
        text_number_of_subscribers = html_tag_with_subscribers.text

        # group here means the found match and we want the first one
        number_of_subscribers_raw = re.search(
            "[\\d\\s]+", text_number_of_subscribers).group(0)
        nos = int(number_of_subscribers_raw.replace(" ", ""))
    except:
        nos = -1
    return nos


def extract_header(soup: BeautifulSoup) -> str:
    try:
        html_tag_with_header = soup.find("div", {"class": "tgme_page_description"})
        header_text = html_tag_with_header.text
    except:
        header_text = ""
    return header_text


def parse_channel_web(channel_name: str) -> int:
    '''
    This parser-function extracts available info about channels from their web-pages generated by telegram.
    Using of HTTP can help us avoiding FloodErrors and bans

    return -1 in case of username is not a channel, channel is private or error
    '''
    try:
        r = requests.get("https://t.me/{}".format(channel_name))
        HTML_content = r.content  # store HTML into the variable content
        soup = BeautifulSoup(HTML_content, "html.parser")
        # nos = extract_subscribers(soup)
        header = extract_header(soup)
    except:
        header = ""
        
    return header


if __name__ == "__main__":
    username = "latinapopacanski"
    # username = "fak_tu"
    # username = "kpotoh"
    ns = parse_channel_web(username)
    print(ns)